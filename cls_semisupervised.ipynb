{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97037c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to read dataset with tuples of image and its name (for eval&test in a later stage)\n",
    "class MYDATA_labeled(Dataset):\n",
    "\n",
    "    def __init__(self, root, label_csv, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = None\n",
    "        df = pd.read_csv(label_csv)\n",
    "        self.img_path_list = []\n",
    "        self.target_list = []\n",
    "        for i in range(df.shape[0]):\n",
    "            self.img_path_list.append(os.path.join(root, df.iloc[i,0]))\n",
    "            self.target_list.append(df.iloc[i,1]-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = Image.open(self.img_path_list[index]).convert(\"RGB\"), self.target_list[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unlabelled data\n",
    "class MYDATA_unlabeled(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = None\n",
    "        self.img_path_list = [os.path.join(root, imgname) for imgname in os.listdir(root)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = Image.open(self.img_path_list[index]).convert(\"RGB\"), -1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84caaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max = 0.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, \"best_loss\")\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, \"best_loss\")\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.val_acc_max < val_acc:\n",
    "            self.val_acc_max = val_acc\n",
    "            self.save_checkpoint(val_loss, model, \"best_acc\")\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, name):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), os.path.join(savedir,'res_{}_checkpoint.pt'.format(name)))\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    \"train\":transforms.Compose([\n",
    "        # transforms.RandomCrop((224, 224), pad_if_needed=True), \n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        # transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.ColorJitter(brightness=[0,0.5], contrast=[0,0.5], saturation=[0,0.5], hue=[0,0.5]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4355, 0.4345, 0.4332], std=[0.2830, 0.2826, 0.2818])\n",
    "    ]),\n",
    "    \"val\":transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4355, 0.4345, 0.4332], std=[0.2830, 0.2826, 0.2818])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model with the best performance to predict the unlabelled data\n",
    "root_dir = \".\"\n",
    "pretrained_model = os.path.join(root_dir, \"checkpoint\", \"run2\", \"res_best_acc_checkpoint.pt\")\n",
    "labeled_dir = os.path.join(root_dir, \"1k_label\")\n",
    "# labeled_dir = os.path.join(root_dir, \"1k6_label\")\n",
    "# labeled_dir = os.path.join(root_dir, \"3k_label\")\n",
    "\n",
    "unlabeled_dir = os.path.join(root_dir, \"29k_unlabel\")\n",
    "val_dir = os.path.join(root_dir, \"testset_4k5\")\n",
    "train_label_csv = os.path.join(root_dir, \"1k_true_label_csv.csv\")\n",
    "# train_label_csv = os.path.join(root_dir, \"1k6_true_label_csv.csv\")\n",
    "# train_label_csv = os.path.join(root_dir, \"all_3k_train_true_label_csv.csv\")\n",
    "test_label_csv = os.path.join(root_dir, \"testset_4k5_true_label_csv.csv\")\n",
    "\n",
    "train_dataset = MYDATA_labeled(labeled_dir, train_label_csv, transform=transform[\"train\"])\n",
    "unlabel_train_dataset = MYDATA_unlabeled(unlabeled_dir, transform=transform[\"train\"])\n",
    "val_dataset = MYDATA_labeled(val_dir, test_label_csv, transform=transform[\"val\"])\n",
    "\n",
    "# Create DataLoaders for the training and validation datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "unlabel_train_dataloader = DataLoader(unlabel_train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = os.path.join(root_dir, \"checkpoint\")\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "exp_id = len(os.listdir(savedir)) + 1\n",
    "savedir = os.path.join(savedir,\"mixlabel\",\"run{}\".format(exp_id))\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "feature = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features=feature,out_features=8,bias=True)\n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load(pretrained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a60d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model with the best performance to predict the unlabelled data\n",
    "basemodel = models.resnet50(pretrained=True)\n",
    "feature = basemodel.fc.in_features\n",
    "basemodel.fc = nn.Linear(in_features=feature,out_features=8,bias=True)\n",
    "basemodel.load_state_dict(torch.load(pretrained_model))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "basemodel = basemodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# base_model = [n for n in list(model.parameters()) if n not in list(model.classifier.parameters())]\n",
    "# optimizer = torch.optim.SGD([\n",
    "#         {\"params\":model.fc.parameters()},\n",
    "#         {\"params\":model.classifier.parameters(),\"lr\":1e-2}\n",
    "#         ],lr = 1e-4, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "num_epochs = 100  # set it to a large number\n",
    "train_iter_num_per_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a201a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel.eval()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    exp_lr_scheduler.step()\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_corrects = 0\n",
    "    print(\"trainning...\")\n",
    "    labeled_train_iter = iter(train_dataloader)\n",
    "    unlabel_train_iter = iter(unlabel_train_dataloader)\n",
    "\n",
    "    for batch_idx in tqdm(range(train_iter_num_per_epoch)):\n",
    "        try:\n",
    "            inputs, labels = labeled_train_iter.next()\n",
    "        except:\n",
    "            labeled_train_iter = iter(train_dataloader)\n",
    "            inputs, labels = labeled_train_iter.next()\n",
    "\n",
    "        try:\n",
    "            u_inputs, u_labels = unlabeled_train_iter.next()\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabel_train_dataloader)\n",
    "            u_inputs, u_labels = unlabeled_train_iter.next()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        u_inputs = u_inputs.to(device)\n",
    "        u_labels = basemodel(u_inputs)\n",
    "        _, u_labels = torch.max(u_labels, 1)\n",
    "\n",
    "        # mixup\n",
    "        all_inputs = torch.cat([inputs, u_inputs], dim=0)\n",
    "        all_targets = torch.cat([labels, u_labels], dim=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(all_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_f(outputs, all_targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * all_inputs.size(0)\n",
    "        running_train_corrects += torch.sum(preds == all_targets.data)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_corrects = 0\n",
    "    print(\"validation...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            running_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / (200*batch_size*2)\n",
    "    epoch_train_acc = running_train_corrects.double() / (200*batch_size*2)\n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "    epoch_val_acc = running_val_corrects.double() / len(val_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} Train Loss: {epoch_train_loss:.4f} Train Acc: {epoch_train_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decreased, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(epoch_val_loss, epoch_val_acc, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
