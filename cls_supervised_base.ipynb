{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to read dataset with tuples of image and its name (for eval&test in a later stage)\n",
    "class MYDATA_labeled(Dataset):\n",
    "\n",
    "    def __init__(self, root, label_csv, transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = None\n",
    "        df = pd.read_csv(label_csv)\n",
    "        self.img_path_list = []\n",
    "        self.target_list = []\n",
    "        for i in range(df.shape[0]):\n",
    "            self.img_path_list.append(os.path.join(root, df.iloc[i,0]))\n",
    "            self.target_list.append(df.iloc[i,1]-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = Image.open(self.img_path_list[index]).convert(\"RGB\"), self.target_list[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max = 0.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, \"best_loss\")\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, \"best_loss\")\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.val_acc_max < val_acc:\n",
    "            self.val_acc_max = val_acc\n",
    "            self.save_checkpoint(val_loss, model, \"best_acc\")\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, name):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), os.path.join(savedir,'res_{}_checkpoint.pt'.format(name)))\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    \"train\":transforms.Compose([\n",
    "        # transforms.RandomCrop((224, 224), pad_if_needed=True), \n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        # transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.ColorJitter(brightness=[0,0.5], contrast=[0,0.5], saturation=[0,0.5], hue=[0,0.5]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4355, 0.4345, 0.4332], std=[0.2830, 0.2826, 0.2818])\n",
    "    ]),\n",
    "    \"val\":transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4355, 0.4345, 0.4332], std=[0.2830, 0.2826, 0.2818])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled dataset\n",
    "root_dir = \".\"\n",
    "# labeled_dir = os.path.join(root_dir, \"1k_label\")\n",
    "# labeled_dir = os.path.join(root_dir, \"1k6_label\")\n",
    "labeled_dir = os.path.join(root_dir, \"3k_label\")\n",
    "\n",
    "unlabeled_dir = os.path.join(root_dir, \"29k_unlabel\")\n",
    "val_dir = os.path.join(root_dir, \"testset_4k5\")\n",
    "\n",
    "# train_label_csv = os.path.join(root_dir, \"1k_true_label_csv.csv\")\n",
    "# train_label_csv = os.path.join(root_dir, \"1k6_true_label_csv.csv\")\n",
    "train_label_csv = os.path.join(root_dir, \"all_3k_train_true_label_csv.csv\")\n",
    "test_label_csv = os.path.join(root_dir, \"testset_4k5_true_label_csv.csv\")\n",
    "\n",
    "train_dataset = MYDATA_labeled(labeled_dir, train_label_csv, transform=transform[\"train\"])\n",
    "val_dataset = MYDATA_labeled(val_dir, test_label_csv, transform=transform[\"val\"])\n",
    "\n",
    "# Create DataLoaders for the training and validation datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "savedir = os.path.join(root_dir, \"checkpoint\")\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "exp_id = len(os.listdir(savedir)) + 1\n",
    "savedir = os.path.join(savedir,\"run{}\".format(exp_id))\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e97654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "feature = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features=feature,out_features=8,bias=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7949a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "# base_model = [n for n in list(model.parameters()) if n not in list(model.classifier.parameters())]\n",
    "# optimizer = torch.optim.SGD([\n",
    "#         {\"params\":model.fc.parameters()},\n",
    "#         {\"params\":model.classifier.parameters(),\"lr\":1e-2}\n",
    "#         ],lr = 1e-4, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "num_epochs = 100  # set it to a large number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    exp_lr_scheduler.step()\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_corrects = 0\n",
    "    print(\"trainning...\")\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_f(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "        running_train_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_corrects = 0\n",
    "    print(\"validation...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            running_val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_dataset)\n",
    "    epoch_train_acc = running_train_corrects.double() / len(train_dataset)\n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "    epoch_val_acc = running_val_corrects.double() / len(val_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} Train Loss: {epoch_train_loss:.4f} Train Acc: {epoch_train_acc:.4f} Val Loss: {epoch_val_loss:.4f} Val Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decreased, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(epoch_val_loss, epoch_val_acc, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
